{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Invariant Representations Tutorial\n",
    "----\n",
    "\n",
    "This is part of an Invariant Representations tutorial series. There's a short intro here to the theory behind what's going on, but for more in depth coverage see [this unimplemented second part](sorry dead link right now). This tutorial requires basic variational auto-encoder knowledge, and some information theory knowledge, but otherwise should hopefully be pretty accessible.\n",
    "\n",
    "If you're not familiar with VAE, there's a great tutorial from Lilian Weng [here](https://lilianweng.github.io/lil-log/2018/08/12/from-autoencoder-to-beta-vae.html).\n",
    "I should find a IT for ML tutorial, but I haven't yet, so if you know of a good one please link me!\n",
    "\n",
    "PLEASE leave comments/advice/requests. I'm new at this, so this is probably sub-optimal, whatever its current state. I'm also not a regular .ipynb user, so even minor things can be PR'd in happily.\n",
    "\n",
    "This tutorial is done in Keras/TF.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Programming Setup\n",
    "----\n",
    "\n",
    "Okay, so we'll need Keras, Numpy, and MNIST data. I'm going to hide all that, but we're going to get $x$ as a big flat vector and $y$ as a one-hot categorical variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "IMG_DIM = 28\n",
    "NUM_LABELS = 10\n",
    "\n",
    "#data comes as images and 1-dim {0,...,9} categorical variable\n",
    "(train_x, train_y), (test_x, test_y) = tf.keras.datasets.mnist.load_data()\n",
    "  \n",
    "#cast and flatten images, renormalizing to [0,1]\n",
    "train_x = train_x.astype(np.float32).reshape( (train_x.shape[0], IMG_DIM**2) ) / 255.0\n",
    "test_x = test_x.astype(np.float32).reshape( (test_x.shape[0], IMG_DIM**2) ) / 255.0\n",
    "\n",
    "#copypaste\n",
    "def one_hot(labels):\n",
    "    num_labels_data = labels.shape[0]\n",
    "    one_hot_encoding = np.zeros((num_labels_data,NUM_LABELS))\n",
    "    one_hot_encoding[np.arange(num_labels_data),labels] = 1\n",
    "    one_hot_encoding = np.reshape(one_hot_encoding, [-1, NUM_LABELS])\n",
    "    return one_hot_encoding\n",
    "\n",
    "train_y = one_hot(train_y).astype(np.float32)\n",
    "test_y = one_hot(test_y).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape)\n",
    "print(train_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a Conditional VAE architecture \n",
    "----\n",
    "<a id=\"arch-construction\"></a>\n",
    "\n",
    "The Conditional VAE is, as its name suggests, a variational auto-encoder with conditional output. This means it should \n",
    "1. Take in $x$ and map it to a latent variable $z$ using the encoder\n",
    "2. Map $z$ to a new $\\hat{x}$ using the decoder.\n",
    "3. Condition (a.k.a. control) the output $\\hat{x}$ by another input $c$, representing specific other factors.\n",
    "\n",
    "In order to do this we'll also set a few constants. We're using ```DIM_Z=2``` for visualization, but feel free to come back and play around with it later. We're also using \"tanh\" activations, but you can come back and change this as well (e.g. to \"relu\").\n",
    "\n",
    "For some this is pretty basic stuff. Skip to [Loss Construction](#loss-construction) if you can do this on your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM_Z = 2\n",
    "DIM_C = NUM_LABELS\n",
    "INPUT_SHAPE = IMG_DIM ** 2\n",
    "ACTIVATION = \"tanh\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we'll build the encoder, which is a two layer fully-connected feed-forward network with two outputs, $z_{mean}$ and $z_{sigma}$. To avoid domain problems, we'll actually output $\\log z_{sigma}$. This means we can use a linear layer instead of having to choose a non-negative activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1203 15:16:10.377006 140196696553280 deprecation_wrapper.py:119] From /data/vision/polina/shared_software/anaconda3_2019.07_py37/envs/dmoyer-default/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W1203 15:16:10.394088 140196696553280 deprecation_wrapper.py:119] From /data/vision/polina/shared_software/anaconda3_2019.07_py37/envs/dmoyer-default/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W1203 15:16:10.419472 140196696553280 deprecation_wrapper.py:119] From /data/vision/polina/shared_software/anaconda3_2019.07_py37/envs/dmoyer-default/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#declare inputs to the encoder, which is just x\n",
    "input_x = keras.layers.Input( shape = [INPUT_SHAPE], name=\"x\" )\n",
    "\n",
    "#first hidden layer\n",
    "enc_hidden_1 = keras.layers.Dense(512, activation=ACTIVATION, name=\"enc_h1\")(input_x)\n",
    "#second hidden layer\n",
    "enc_hidden_2 = keras.layers.Dense(512, activation=ACTIVATION, name=\"enc_h2\")(enc_hidden_1)\n",
    "\n",
    "#first output, z_mean\n",
    "z_mean = keras.layers.Dense(DIM_Z, activation=ACTIVATION)(enc_hidden_2)\n",
    "#second hidden output, z_log_sigma_sq.\n",
    "z_log_sigma_sq = keras.layers.Dense(DIM_Z, activation=\"linear\")(enc_hidden_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the latent variable $z$ using a Gaussian layer can be a bit tricky, but luckily is covered in the [Keras documentation](https://keras.io/examples/variational_autoencoder/), which this part follows almost exactly. If you haven't seen the reparameterization trick before, this is to create a Gaussian distributed layer $z$ which can be differentiated w.r.t. its parameters. It was popularized by [Kingma and Welling 2014](link here). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1203 15:16:14.491023 140196696553280 deprecation_wrapper.py:119] From /data/vision/polina/shared_software/anaconda3_2019.07_py37/envs/dmoyer-default/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4115: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#stolen straight from the docs\n",
    "#https://keras.io/examples/variational_autoencoder/\n",
    "def sampling(args):\n",
    "    \"\"\"Reparameterization trick by sampling from an isotropic unit Gaussian.\n",
    "\n",
    "    # Arguments\n",
    "        args (tensor): mean and log of variance of Q(z|X)\n",
    "\n",
    "    # Returns\n",
    "        z (tensor): sampled latent vector\n",
    "    \"\"\"\n",
    "\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    # by default, random_normal has mean = 0 and std = 1.0\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "\n",
    "z_mean = keras.layers.Dense(DIM_Z, activation=\"tanh\")(enc_hidden_2)\n",
    "z_log_sigma_sq = keras.layers.Dense(DIM_Z, activation=\"linear\")(enc_hidden_2)\n",
    "\n",
    "z = keras.layers.Lambda(sampling, output_shape=(DIM_Z,), name='z')([z_mean, z_log_sigma_sq])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, that's the Encoder from $x$ to $z$ done. Let's build the conditional decoder. There isn't an established standard way to condition the output, but concatenating $z$ and $c$ before inputting into the decoder is good enough for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#declare any additional inputs to our decoder, in this case c\n",
    "input_c = keras.layers.Input( shape = [DIM_C], name=\"c\")\n",
    "#this is the concat operation!\n",
    "z_with_c = keras.layers.concatenate([z,input_c])\n",
    "\n",
    "#first hidden layer\n",
    "dec_hidden_1 = keras.layers.Dense(512, activation=ACTIVATION, name=\"dec_h1\")(z_with_c)\n",
    "#second hidden layer\n",
    "dec_hidden_2 = keras.layers.Dense(512, activation=ACTIVATION, name=\"dec_h2\")(dec_hidden_1)\n",
    "\n",
    "#output, should be same domain as x_hat\n",
    "#could also use sigmoid activation\n",
    "x_hat = keras.layers.Dense( INPUT_SHAPE, name=\"x_hat\", activation=\"linear\" )(dec_hidden_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss Construction for Invariance (...and also all that VAE stuff)\n",
    "----\n",
    "<a id=\"loss-construction\"></a>\n",
    "\n",
    "Okay, so we have both the encoder and the conditional decoder now, so the next step is building the loss function. There are three sub-components:\n",
    "\n",
    "1. Reconstruction (how far is $\\hat{x}$ from $x$), usually $\\|x - \\hat{x}\\|_2^2$.\n",
    "2. \"Distance\" to the Prior (from the original VAE definition), $KL[q(z|x)| p(z)]$\n",
    "3. \"Distance\" to the Empirical Marginal Posterior (from our paper, among others), $KL[q(z|x)| q(z)]$.\n",
    "\n",
    "The third one we'll have to approximate, so we'll deal with that second. First, the two easy ones, straight from Keras Docs, plus defining some hyper parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "  \"beta\" : 0.1,\n",
    "  \"lambda\" : 1.0,\n",
    "}\n",
    "\n",
    "recon_loss = keras.losses.mse(input_x, x_hat)\n",
    "recon_loss *= INPUT_SHAPE #optional, in the tutorial code though\n",
    "\n",
    "prior_loss = 1 + z_log_sigma_sq - K.square(z_mean) - K.exp(z_log_sigma_sq)\n",
    "prior_loss = K.sum(prior_loss, axis=-1)\n",
    "prior_loss *= -0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for $KL[ q(z|x) | q(z) ]$. Since we're using the Gaussian $z$ layer, there's an approximation we can make using the pairwise Gaussian KL divergences. **In the original version of the paper there is an erroneous extra term at this part.** The corrected version is implemented below, but before we get into it, we should remember that we actually want to compute $KL[ q(z|x) | q(z) ]$, not this bound, and that this could also be solved using:\n",
    "\n",
    "* Direct approximation (e.g. use a neural network to approximate posterior marginal $q(z)$ term given $q(z|x)$ parameters and samples $x$).\n",
    "* Sampling\n",
    "* Use a different $z$ layer with analytic divergence to its marginal.\n",
    "\n",
    "The first and second options appear in the literature; see e.g. [Fixing a Broken ELBO (Alemi et al. 2017)](https://arxiv.org/abs/1711.00464) and [Structured Disentangled Representations Esmaeili et al. 2018](https://arxiv.org/abs/1804.02086). The third option appears in our paper [Echo Noise](https://arxiv.org/abs/1904.07199), which we'll demonstrate at the end.\n",
    "\n",
    "Knowing this, here's one way to do it *for Gaussian $z$ layers* using pairwise KL divergence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#KL(N_0|N_1) = tr(\\sigma_1^{-1} \\sigma_0) + \n",
    "#  (\\mu_1 - \\mu_0)\\sigma_1^{-1}(\\mu_1 - \\mu_0) - k +\n",
    "#  \\log( \\frac{\\det \\sigma_1}{\\det \\sigma_0} )\n",
    "def all_pairs_gaussian_kl(mu, sigma, add_third_term=False):\n",
    "    sigma_sq = tf.square(sigma) + 1e-8\n",
    "    sigma_sq_inv = tf.reciprocal(sigma_sq)\n",
    "\n",
    "    #dot product of all sigma_inv vectors with sigma is the same as a matrix mult of diag\n",
    "    first_term = tf.matmul(sigma_sq,tf.transpose(sigma_sq_inv))\n",
    "    \n",
    "    r = tf.matmul(mu * mu,tf.transpose(sigma_sq_inv))\n",
    "    r2 = mu * mu * sigma_sq_inv \n",
    "    r2 = tf.reduce_sum(r2,1)\n",
    " \n",
    "    #squared distance\n",
    "    #(mu[i] - mu[j])\\sigma_inv(mu[i] - mu[j]) = r[i] - 2*mu[i]*mu[j] + r[j]\n",
    "    #uses broadcasting\n",
    "    second_term = 2*tf.matmul(mu, tf.transpose(mu*sigma_sq_inv))\n",
    "    second_term = r - second_term + tf.transpose(r2)\n",
    "\n",
    "    # log det A = tr log A\n",
    "    # log \\frac{ det \\Sigma_1 }{ det \\Sigma_0 } =\n",
    "    #   \\tr\\log \\Sigma_1 - \\tr\\log \\Sigma_0 \n",
    "    # for each sample, we have B comparisons to B other samples...\n",
    "    #   so this cancels out\n",
    "\n",
    "    if(add_third_term):\n",
    "        r = tf.reduce_sum(tf.log(sigma_sq),1)\n",
    "        r = tf.reshape(r,[-1,1])\n",
    "        third_term = r - tf.transpose(r)\n",
    "    else:\n",
    "        third_term = 0\n",
    "\n",
    "    #- tf.reduce_sum(tf.log(1e-8 + tf.square(sigma)))\\\n",
    "    # the dim_z ** 3 term comes fro\n",
    "    #   -the k in the original expression\n",
    "    #   -this happening k times in for each sample\n",
    "    #   -this happening for k samples\n",
    "    #return 0.5 * ( first_term + second_term + third_term - dim_z )\n",
    "    return 0.5 * ( first_term + second_term + third_term )\n",
    "\n",
    "#\n",
    "# kl_conditional_and_marg\n",
    "#   \\sum_{x'} KL[ q(z|x) \\| q(z|x') ] + (B-1) H[q(z|x)]\n",
    "#\n",
    "\n",
    "#def kl_conditional_and_marg(args):\n",
    "def kl_conditional_and_marg(z_mean, z_log_sigma_sq, dim_z):\n",
    "    z_sigma = tf.exp( 0.5 * z_log_sigma_sq )\n",
    "    all_pairs_GKL = all_pairs_gaussian_kl(z_mean, z_sigma, True) - 0.5*dim_z\n",
    "    return tf.reduce_mean(all_pairs_GKL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So after all that, we can create our third loss term. We'll then add them all up, and create our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1203 15:17:10.308701 140196696553280 deprecation_wrapper.py:119] From /data/vision/polina/shared_software/anaconda3_2019.07_py37/envs/dmoyer-default/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kl_qzx_qz_loss = kl_conditional_and_marg(z_mean, z_log_sigma_sq, DIM_Z)\n",
    "\n",
    "#Invariant Conditional Variational Autoencoder (ICVAE).\n",
    "#I think the name game is meh (what if someone else got there first? or defines a different one later?)\n",
    "#so maybe it'd be easier to say Moyer 2018? too narcissistic? Conditional VAE with additional compression?\n",
    "#\n",
    "# ...the point is, this one can induce invariance. So can the others sometimes, too, in practice.\n",
    "\n",
    "icvae_loss = K.mean((1 + params[\"lambda\"]) * recon_loss + params[\"beta\"]*prior_loss + params[\"lambda\"]*kl_qzx_qz_loss)\n",
    "\n",
    "icvae = keras.models.Model(inputs=[input_x,input_c], outputs=x_hat, name=\"ICVAE\")\n",
    "\n",
    "icvae.add_loss(icvae_loss)\n",
    "\n",
    "learning_rate = 0.0005\n",
    "opt = keras.optimizers.Adam(lr=learning_rate)\n",
    "\n",
    "icvae.compile( optimizer=opt, )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Only run this bit once. It takes a bit (but not too long tbh). If this isn't your jupyter server, you should delete the path stuff. I just wanted to save you from repetitive gpu time/coffee breaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists(\"mnist_icvae.h5\"):\n",
    "  cvae.fit(\n",
    "    { \"x\" : train_x, \"c\" : train_y }, epochs=100\n",
    "  )\n",
    "  cvae.save_weights(\"mnist_icvae.h5\")\n",
    "else:\n",
    "  cvae.load_weights(\"mnist_icvae.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots and Evaluation!\n",
    "\n",
    "Okay, so now that we've trained it, let's see how well it does!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_plot_samps = 10\n",
    "test_x_hat = mean_cvae.predict(\n",
    " { \"x\" : test_x[:n_plot_samps], \"c\" : test_y[:n_plot_samps] }\n",
    ")\n",
    "\n",
    "##\n",
    "## plot first N\n",
    "##\n",
    "\n",
    "from plot_tools import pics_tools as pic\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = pic.plot_image_grid( \\\n",
    "  np.concatenate([test_x[:n_plot_samps],test_x_hat], axis=0),\n",
    "  [mnist_dataset.IMG_DIM, mnist_dataset.IMG_DIM], \\\n",
    "  (2,n_plot_samps) \\\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
